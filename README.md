# ğŸ§  Reinforcement Learning for Adaptive Learning (RL_for_AL)

This project applies **Reinforcement Learning** (specifically **Proximal Policy Optimization - PPO**) to an **Adaptive Learning environment**. The RL agent learns to personalize educational activities for students with different learning styles and velocities.

---

### âš™ï¸ Setup with Poetry

This project uses [Poetry](https://python-poetry.org/) for dependency and virtual environment management.

### ğŸ“ Project Strucutre
RL_for_AL/
â”‚
â”œâ”€â”€ data/ # Curriculum datasets (JSON)
â”œâ”€â”€ environments/ # Custom Gym environment
â”‚ â””â”€â”€ AdaptiveLearningEnv.py
â”œâ”€â”€ models/ # PPO agent implementation
â”‚ â”œâ”€â”€ checkpoints/ # Saved model weights
â”‚ â””â”€â”€ ppo_model/
â”‚ â”œâ”€â”€ ActorCritic.py
â”‚ â”œâ”€â”€ PPO.py
â”‚ â””â”€â”€ utils.py
â”œâ”€â”€ tests/ # Unit and integration tests
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ test.py
â”œâ”€â”€ training/ # Training configuration & scripts
â”‚ â”œâ”€â”€ config.yaml
â”‚ â””â”€â”€ train.py
â”œâ”€â”€ utils/ # Data loading, logging, plotting
â”‚ â”œâ”€â”€ config_loader.py
â”‚ â”œâ”€â”€ data_generator.py
â”‚ â”œâ”€â”€ load_dataset.py
â”‚ â”œâ”€â”€ logger.py
â”‚ â””â”€â”€ plot_rewards.py
â”œâ”€â”€ logs/ # Training and testing logs (.csv)
â”œâ”€â”€ pyproject.toml # Poetry configuration
â”œâ”€â”€ poetry.lock
â””â”€â”€ README.md # Project description

### 1. Clone the repository

```bash
git clone https://github.com/Herriot65/A-Reinforcement-Learning-Algorithm-for-Adaptive-Learning.git
cd RL_for_AL
````

### 2. Install dependencies

```bash
poetry install
```

### 3. Activate the virtual environment

```bash
poetry shell
```

---

## ğŸš€ How to Train the Agent

Run the PPO training loop using:

```bash
python training/train.py
```

Training logs will be saved to:

```
logs/training_logs_XXXX.csv
```

Model checkpoints are saved in:

```
models/checkpoints/ppo_model.pt
```

---

## ğŸ§ª How to Test the Agent

Evaluate the trained agent on synthetic student profiles:

```bash
python scripts/test.py
```

This will:

* Generate random students (style + velocity)
* Run the trained PPO model
* Save results to: `logs/test_results.csv`

---

## ğŸ“Š Plotting Episode Rewards

To visualize the reward evolution during training:

```python
from utils.plot_rewards import plot_rewards_from_csv

plot_rewards_from_csv("logs/training_logs_XXXX.csv")
```

---

## ğŸ“¦ Configuration

All training hyperparameters are stored in `config.yaml`:

```yaml
n_episodes: 5000
LR: 0.0003
GAMMA: 0.99
LAMDA: 0.95
EPOCHS: 10
CLIP_EPSILON: 0.2
entropy_coef: 0.01
minibatch_size: 64
freq_update: 2048
```

This file is loaded automatically during training and testing. You can modify these parameters.

---

## ğŸ“š Dataset

Dataset follows this structure:

```json
{
  "lessons": {...},
  "activities": {...},
  "sprints": {...}
}
```

